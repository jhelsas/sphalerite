

​	Also, an **efficient** implementation is not solely a tweak over the original calculation, only focusing on how to execute the computation itself faster: rethinking what is the actual problem to be solved, exactly which hypothesis are being used and how they can be **exploited** for speed is a major aspect of numerical code optimization: If $W(|{\mathbf x}_i - {\mathbf x}_j|, h)$ was a long ranged kernel, such as $W(|{\mathbf x}_i - {\mathbf x}_j|, h) = \frac{1}{|{\mathbf x}_i - {\mathbf x}_j|}$ instead of a kernel of compact support, cutting off all computations of particles farther then a given radius would introduce a substantial amount of error in the computation, and would ultimately damage the result beyond usability. 

​	To be able to speedup this calculation, contributions from further away particles need to be taken into account, but since the kernel decreases with distance, small differences in far away particle positions don't affect too much the final result. This observation, or rather its [precise mathematical formulation](https://en.wikipedia.org/wiki/Multipole_expansion), is the basis for the tree-based techniques for accelerating gravitational and molecular-dynamics [n-body simulations](https://en.wikipedia.org/wiki/N-body_simulation), like the [Barnes-Hut tree](https://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation) and the [Fast Multipole Method](https://en.wikipedia.org/wiki/Fast_multipole_method), which utilize coarse grained data over an hierarchical representation of the domain in order to obtain fast aggregate approximations for this kind of summations in the case of long range potentials and forces. What these "macro" optimizations effectively do is **change the complexity of the algorithm** to obtain the desired result, therefore enabling realistic scenarios. In the example of the Cell Linked List from the previous article, what that algorithm does is reduce the effective complexity of the pairs computation from ${\mathcal O}(N^2)$ always to something between ${\mathcal O}(N)$ and ${\mathcal O}(N^{4/3})$  most of the time which, as seen, is much faster. 

## Modern CPUs and the Rise of Parallelism

​	So, if you did the course I mentioned in the introduction (I am not being paid by them), you will have learned that modern CPU architectures exploit several levels of parallelism in order to achieve their full perform, in contrast to what was the norm till the early 2000s before which the entire focus was on increasing clock speed of the processors. These were the heyday of the Pentium 3 processor, before the multi-core era started with the Core2Duo and the Athlon X2 CPUs in the desktop. To understand what happened, and how this affects you, it is necessary to make a dive on how CPUs work, what this means in terms of software, and how you need to interact with it. Sorry if it feels cumbersome, but I promise it will be worth it. 

​	In that era, if you code didn't run fast enough for what you wanted all you needed to do was to wait a couple of years for the performance to double or triple and all was fine. Unfortunately, that age is over, at least somewhat. The reason is that most computer code was written to be executed a single stream of sequentially dependent instructions, otherwise known as *thread*, which fed an ever faster CPU core that was able to easily fetch data from Ram, the only barrier (mostly) being how fast the computation could be performed. Since then, CPU design bumped in a roadblock that turned into a rock wall, related to how high clock frequencies can be pushed. 

​	![core_counts_cpu_perf](/home/jhelsas/Manuscripts/Medium-publication/core_counts_cpu_perf.png)

​	Extracted from [Wikipedia]()

​	Essentially the frequency of CPU, and to a very large extent the performance per core, have stagnated since the mid 2000s because if the chip goes much faster it will [simply melt](https://www.researchgate.net/publication/220110174_Green_Secure_Processors_Towards_Power-Efficient_Secure_Processor_Design/figures?lo=1), this is usually called "The Power Wall" for CPUs. Nevertheless the number of transistors keeps increasing exponentially, if they are not being used to make *beefier*, *smarter*, *faster* cores they must go **somewhere**, and the somewhere else is what makes programming comparatively more difficult for developers now a days: Instead of executing one instruction stream faster, the additional transistors are being used to execute **multiple operations in multiple instructions streams, all in parallel**.

​	These multiple streams shows up in several aspects in modern CPUs: [Out-of-order execution](https://en.wikipedia.org/wiki/Out-of-order_execution) can extract additional single core performance by executing independent instructions in the way that minimizes stalling and/or [execute them speculatively](https://en.wikipedia.org/wiki/Speculative_execution) Also, modern processing cores use multiple independent processing units to perform independent operations coming from a single instruction in parallel, therefore being classified as [Superscalar architectures](https://en.wikipedia.org/wiki/Superscalar_processor). When you arrive in a modern day processor, this implies something like the **absolute beast** shown below, which have 4 [ALU](https://en.wikipedia.org/wiki/Arithmetic_logic_unit)s for computing up to 4 independent integer instructions per core per clock, and 3 [AGU](https://en.wikipedia.org/wiki/Address_generation_unit)s for computing memory addresses of data that needs to be used. Still, this particular part is hidden from the user so, from the point of the view of the developer, this represents "free" performance gains ([at least most of the time](https://en.wikipedia.org/wiki/Branch_predictor)). Unfortunately, the rest of the performance benefits coming from the additional transistors **do not come nearly as easily** as these. 

![Zen2 Micro-architecture Anandtech](https://images.anandtech.com/doci/14525/Mike_Clark-Next_Horizon_Gaming-CPU_Architecture_06092019-page-003.jpg)

Extracted from [Anandtech](https://www.anandtech.com/show/14525/amd-zen-2-microarchitecture-analysis-ryzen-3000-and-epyc-rome/6).

​	Much of the additional transistor budget available is in used one of 3 ways: Increasing the **number of independent instruction streams** that can be processed, enabling a single instruction stream to perform **more operations using the same amount of instructions** and increasing the amount of **fast memory inside the core** to keep all this computation going. 

### A New CPU Consciousness

​	To increase the number of instructions streams, modern CPUs use several "mini-CPU" inside them, called **cores**, that usually are all copies of each other but [not always](https://en.wikipedia.org/wiki/ARM_big.LITTLE). This allows for multiple processes to be executed simultaneously in the CPU, and answer for much of the performance gains in recent years, but from the developer point of view, this means that your code much explicitly send instructions to multiple available **logical cores called threads**, in order to feed all the available hardware with work to be done. This is implemented using a thread parallelism library, such as pthreads or openMP, the latter being the chosen option for this project due to the simpler nature for the kind of code we are dealing with. 

​	Modern CPUs also have multiple instruction streams feeding the same physical core in the hardware level, using something called [Simultaneous Multi-Threading](https://en.wikipedia.org/wiki/Simultaneous_multithreading)(SMT) which fulfills a slightly different purpose than the existence of multiple cores: Modern CPUs have so many processing available inside their cores that can be difficult to have enough instructions to keep them all busy, so having multiple streams to **draw instructions from** can help with with **hardware utilization**, but it does not increase the maximum potential performance the core itself could have. It can feel strange from the developer point of view because SMT simply appears as **additional cores** being available to the Operational System, without any particular distinction, but understanding this difference might be important to your particular application since [some applications benefit](https://www.nersc.gov/assets/pubs_presos/CUG13HTpaper.pdf) from having more threads sharing a core, and [others simply don't](https://www.cfd-online.com/Forums/openfoam/101434-openfoam-hyperthreading.html). 

​	If you are wandering what a modern CPU looks like, this is a die shot of a slightly newer CPU from the same vendor as mine, with each part highlighted according to its functionality:

![](https://cdn.wccftech.com/wp-content/uploads/2020/11/AMD-Ryzen-5000-Zen-3-Desktop-CPU_Vermeer_Die-Shot_1-scaled.jpg)

​	In the image above, each Zen 3 core "effectively" operates as an quasi-independent CPU, executing its own instructions, asking for data of its own interest unless otherwise stated. You can think of each core as members of a Kibbutz called the Core Complex (CCX), which is formed by this block connecting to the rest of the CPU and the out-side world. 

### The Return of the Vector Processor

​	Also, if more operations can be done for each instruction issued to the core, more performance can be obtained by mowing through the data. This idea hearkens back to the first Supercomputer, the [LANL Cray-1](https://en.wikipedia.org/wiki/Cray-1) vector machine, and have different showed up in different iterations throughout computer history including modern [vector processors](https://en.wikipedia.org/wiki/NEC_SX-Aurora_TSUBASA), present day [Graphic Processing Units](https://en.wikipedia.org/wiki/Vector_processor#GPU) in general with a particular poignant example present in the newer [Tensor Cores](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/), and more to our present interest this idea is manifested in the existence of [Single Instruction, Multiple Data](https://en.wikipedia.org/wiki/SIMD) instruction and hardware present in modern CPUs. 

​	The intuition behind the SIMD is relatively simple: Many codes, specially numerical code, are constituted for fairly large iterations of equal or very similar operations, many times only using basic floating-point or integer arithmetic. If many of these operations can be coalesced together, e.g. because they operate in independent data, it is possible to issue a single instruction that perform 2, 4 or even 8 of these operations *at once*, therefore promising a big speedup. In comparison, multiple cores offer a different kind of parallelism called Multiple Instruction Multiple Data (MIMD). The overall idea looks like this:

​	![https://www.techrepublic.com/article/the-web-takes-a-step-closer-to-becoming-a-computing-platform-with-simd/](https://www.techrepublic.com/a/hub/i/2014/09/16/e6400e0e-96b4-42c1-95be-79cd7e939321/simd-example.jpg)

​	Though the promise is fairly straightforward, actually making good on this promise anything but. The reasons for it will be explored in more detail throughout the rest of this work but the first road-block begins with the fact that, for the SIMD units to work properly the CPU **expects** that the data that will be operated on is laid out in memory in a way that consecutive elements in memory **correspond to consecutive operands** in the instruction. What this means is that if you have particle $i$ with position `x` , it expects that the memory is laid out like a conventional array `...x[i], x[i+1], x[i+2], ...` and not in a strided fashion as occurs with the original layout we used in the struct `... x[i], y[i], z[i], ... rho[i], x[i+1], y[i+1], z[i+1], ...`. 

​	Just that modification, if not done early, [can be quite the headache to do](https://arxiv.org/abs/1612.06090v1). Since I was aware of this quite early in the project, I was capable of investigating the impact of the necessary changes in order to see if I was able to obtain substantial performance gains from SIMD, which ultimately did happen. The actual code doesn't look too different but the function calls might, and that is why it can be cumbersome to do this migration. Also, some arcane hardware features like [memory alignment](https://en.wikipedia.org/wiki/Data_structure_alignment) suddenly become relevant for the computational scientist, and this can be irritating to deal with since it can be hardware dependent. 

### The Memory Bandwidth Strikes Back

​	The last piece of the puzzle is the **fast memory** needed to feed all the machinery discussed above. To understand why we need the fast memory inside the core, called **cache**, it is first necessary to understand why the processor needs to be **fed with data**. If you look back in the "Zen 2 Micro-Architecture", it possesses two pairs of `MUL` and `ADD` blocks. These blocks are capable of executing [4 blocks of operations](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#Advanced_Vector_Extensions_2) in the form `a = b*c+a` called [3 operand fused multi-add](https://en.wikipedia.org/wiki/FMA_instruction_set#Instructions) (FMA3) on 64-bit floating point data per clock, per CPU core. This means each CPU core can, theoretically, perform 8 operations (4 additions and 4 multiplicaions) in different `a`, `b` and `c` data *every **single** cycle*. In our case, this can be estimated as being $12\ \mbox{cores} \times 4.0\ \mbox{GHz}\ \times 2\ \times 2 \times 4\ \mbox{ops/core}\approx 768\ \mbox{GFlops}$. It looks big, but the requirements to do so are not big, they are almost *obscene*. 

​	To perform this computation, it needs to load 3 double precision floating point numbers from memory for each operation, therefore it requires $12\ \mbox{cores}\times 4.0\ \mbox{GHz}\ \times 2 \times 4\ \mbox{ops/core}\ \times 3\ \mbox{double/op} \times 8\ \mbox{bytes/double}\approx 9.2\ \mbox{TB/s}$, which is quite the astounding number in case you are not used to seeing these bandwidth figures. As a comparison, my main system memory can only provide $45-50\ \mbox{GB/s}$  of bandwidth, so there is no way it would be possible to provide all the data needed to keep this many operations pulling data just from main memory, it is **simply not fast enough**. For comparison, a fast consumer grade SSD can do $\sim 3-7\ \mbox{GB/s}$ of bandwidth and an old hard-drive can only provide $0.125 - 0.250\ \mbox{GB/s}$. And yet, AMD spent a lot of money to develop it the silicon to perform it and I paid a fair amount myself to purchase a CPU that has it, therefore the must be a way to use it, and **indeed there is**.

​	The way to do it is through the **fast in-chip memory** named cache(s). The closest cache to the computing units, the L1 cache, is able to transfer 32 bytes/cycle to the [registers](https://en.wikipedia.org/wiki/Processor_register), which are where the data is needed to perform the operations. Computing the aggregate memory bandwidth provided by the L1 cache from all 12 cores, the **potential aggregate L1 bandwidth** adds to $12 \times 32\ \mbox{B/(cycle * port)} \times 3\ \mbox{port} \times\ 4.0\ \mbox{GHz} \approx 3.072\ \mbox{TB/s}$, which is not quite yet what we would theoretically need to fully execute **arbitrary** FMA3 instructions in the CPU, but it is not difficult to see how data re-use could help bridge the last $3 \times$ factor, which **would be the case** in many applications such as ours because there is a natural data re-use in the form of the reduction inherent to the calculation of the distance, and it is also present in the reduction used in the density calculation, which is essentially a reduction computation for each particle. In this sense, the cache don't need to provide the bandwidth for **all** FMA3 computations, but only for the **new** data needed to perform additional computations. 

​	For anyone wondering why such huge discrepancy between the computational capability of the CPU and the availability of memory resources, this issue is know as the [Gap between Processor and Memory Speeds](https://www.semanticscholar.org/paper/The-Gap-between-Processor-and-Memory-Speeds-Carvalho/6ebec8701893a6770eb0e19a0d4a732852c86256). The graph below shows the relative increase in performance of CPUs and DRAM between the years of 1980 and 2000, and the chasm gets bigger 50% a year, and it shows no sign of going the other way around. 

![https://www.semanticscholar.org/paper/The-Gap-between-Processor-and-Memory-Speeds-Carvalho/6ebec8701893a6770eb0e19a0d4a732852c86256/figure/0](https://d3i71xaburhd42.cloudfront.net/6ebec8701893a6770eb0e19a0d4a732852c86256/1-Figure1-1.png)

​	The solution found to bridge this gap was to introduce a variety of intermediary memories **in between** the storage the CPU uses to perform calculations, called the **register**, and the **main system memory**. These memories are called caches, and are numbered according to how far away they are from the actual processing units, ALU and FPU, and therefore how large and slower they are. Level 1 cache (L1) is the fastest in-chip memory, and L3 is usually the slowest. They have varying sizes and speed, but they are always **smaller and faster** than main memory. Most of the time they are automatically managed by the CPU itself, but in some processors [they can be programmable](https://en.wikipedia.org/wiki/Scratchpad_memory), requiring you to program. The most widely know example is Nvidia Cuda's Shared Memory, which corresponds to a programmable L1 cache. 

​														![](https://d3i71xaburhd42.cloudfront.net/6ebec8701893a6770eb0e19a0d4a732852c86256/3-Figure2-1.png)

​	To a large extent, extracting the full potential performance nowadays means managing this **hierarchy of memory** in the way you code. Many of the more refined, arcane and confusing techniques for improving numerical code performance are focused exactly in improving cache utilization and re-use through emphasis in [data locality](https://en.wikipedia.org/wiki/Locality_of_reference), both spatially as discussed with the striding, and temporally with reuse of results. This overall philosophy is sometimes known as [Data-Oriented Programming/Design](https://en.wikipedia.org/wiki/Data-oriented_design), and it focus on exploiting a mixture of traditional task parallel and newer [data parallel](https://en.wikipedia.org/wiki/Data_parallelism) techniques to achieve the maximum available performance. Some of these techniques will be the subject of later sections. And as such, lets turn to one of them. 

### 	TLDR: 

​	CPUs now a days can do a lot of work inside a core and between cores. It is **the developer's job** to find ways of mapping the application's abstractions into the available resources, may they be threads, SIMD units or other available computing resources. All this computing demands an astounding amount of data, and to **keep the beast fed**, it is necessary to utilize fast memory near the CPU to store **temporary data** used in calculation and **re-use this data as much and as possible**. Writing your program in a way that is friendly to the way the hardware works goes a long way into facilitating this process. 

Appendix: pthreads, C99 threads and other thread libraries

The second reason this layout is not optimal is related to the way SIMD hardware works. To utilize the SIMD units, the compiler converts the loop in a fashion that is similar to the decomposition below:

```C
for(int64_t jj=0;jj<N;jj+=SIMD_WIDTH)
	for(int64_t j=jj;j<jj+SIMD_WIDTH;j+=1)
```

​	After making this "conversion", the compiler re-writes the inner-most, fixed-length, loop as a single bunch of instructions that do **the equivalent work of the entire loop**. These operations are done in parallel inside the SIMD units, which is great, but they come with some **quirks**. For most CPUs, the compiler expects that the operations are **independent of each other**, the data that is used for the loop is found **sequentially in memory for the same operand** and in an easy, **aligned and non-aliased** way. If any of these **very restrictive** conditions are violated, either you don't get *vectorized*, i.e. SIMD enabled, code or it will execute incorrectly. In any case, these are details for the later section, but, none of this can begin before fixing the data layout in memory.

